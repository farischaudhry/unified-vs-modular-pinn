{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDESystem and Network Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\n",
    "    ((64, 64), \"SmallEqual\"),\n",
    "    ((128, 64), \"Baseline\"),\n",
    "    ((128, 64, 32), \"MediumThreeTapered\"),\n",
    "    ((64, 64, 64), \"MediumThreeEqual\"),\n",
    "    ((64, 128, 128, 64), \"MidInverseTapered\"),\n",
    "    ((256, 256), \"MediumEqual\"),\n",
    "    ((64, 64, 64, 64), \"MediumDeepEqual\"),\n",
    "    ((512, 256, 128, 64), \"LargeTapered\"),\n",
    "    ((256, 256, 256, 256), \"LargeEqual\"),\n",
    "]\n",
    "\n",
    "total_params = [ # as calculated by TensorFlow\n",
    "    ((64, 64), 4547),\n",
    "    ((128, 64), 8835),\n",
    "    ((128, 64, 32), 10819),\n",
    "    ((64, 64, 64), 8707),\n",
    "    ((64, 128, 128, 64), 33475),\n",
    "    ((256, 256), 67331),\n",
    "    ((64, 64, 64, 64), 12867),\n",
    "    ((512, 256, 128, 64), 174211),\n",
    "    ((256, 256, 256, 256), 198915),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title PDESystem\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "import time\n",
    "import shap\n",
    "import os\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "class PDESystem:\n",
    "    '''\n",
    "    Abstract class for defining a PDE system.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, num_components: int, x_domains=[(0, 1)], t_domain=(0,1)):\n",
    "        \"\"\"\n",
    "        Initialize the PDE system with the number of components and domain ranges.\n",
    "\n",
    "        Parameters:\n",
    "        num_components (int): Number of components in the PDE system.\n",
    "        x_domains (list): List of tuples defining the spatial domain ranges.\n",
    "        t_domain (tuple): Tuple defining the temporal domain range.\n",
    "        \"\"\"\n",
    "        self.num_components = num_components\n",
    "        self.x_domains = x_domains\n",
    "        self.t_domain = t_domain\n",
    "\n",
    "    @abstractmethod\n",
    "    def residuals(self, model: tf.keras.Model, t_points: tf.Tensor, x_points: tf.Tensor):\n",
    "        \"\"\"\n",
    "        Abstract method to compute the residuals of the PDE system.\n",
    "\n",
    "        Parameters:\n",
    "        model (tf.keras.Model): The neural network model.\n",
    "        t_points (tf.Tensor): Tensor of temporal points.\n",
    "        x_points (tf.Tensor): Tensor of spatial points.\n",
    "\n",
    "        Returns:\n",
    "        List[tf.Tensor]: List of residuals for each component.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def compute_bounary_loss(self, model: tf.keras.Model, t_points: tf.Tensor, x_points: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the boundary loss for the PDE system.\n",
    "\n",
    "        Parameters:\n",
    "        model (tf.keras.Model): The neural network model.\n",
    "        t_points (tf.Tensor): Tensor of temporal points.\n",
    "        x_points (tf.Tensor): Tensor of spatial points.\n",
    "\n",
    "        Returns:\n",
    "        tf.Tensor: Boundary loss.\n",
    "        \"\"\"\n",
    "        return tf.constant(0.0)\n",
    "\n",
    "    def compute_loss(self, model: tf.keras.Model, t_points: tf.Tensor, x_points: tf.Tensor, lambda_bc: float = 1) -> Tuple[tf.Tensor, List[tf.Tensor]]:\n",
    "        \"\"\"\n",
    "        Compute the overall loss and component losses for the PDE system.\n",
    "\n",
    "        Parameters:\n",
    "        model (tf.keras.Model): The neural network model.\n",
    "        t_points (tf.Tensor): Tensor of temporal points.\n",
    "        x_points (tf.Tensor): Tensor of spatial points.\n",
    "        lambda_bc (float): Weight for the boundary condition loss.\n",
    "\n",
    "        Returns:\n",
    "        Tuple[tf.Tensor, List[tf.Tensor]]: Overall loss and list of component losses.\n",
    "        \"\"\"\n",
    "        res_list = self.residuals(model, t_points, x_points)\n",
    "        res_concat = tf.concat(res_list, axis=1)\n",
    "\n",
    "        res_concat = tf.cast(res_concat, dtype=tf.float64)\n",
    "        overall_loss = tf.reduce_mean(tf.square(res_concat)) + tf.cast(lambda_bc, dtype=tf.float64) * tf.cast(\n",
    "            self.compute_bounary_loss(model, t_points, x_points), dtype=tf.float64)\n",
    "\n",
    "        component_losses = [tf.reduce_mean(tf.square(tf.cast(res, dtype=tf.float64))) for res in res_list]\n",
    "        return tf.cast(overall_loss, dtype=tf.float32), [tf.cast(loss, dtype=tf.float32) for loss in component_losses]\n",
    "\n",
    "    def generate_points(self, N: int = 1000) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        \"\"\"\n",
    "        Generate random points within the spatial and temporal domains.\n",
    "\n",
    "        Parameters:\n",
    "        N (int): Number of points to generate.\n",
    "\n",
    "        Returns:\n",
    "        Tuple[tf.Tensor, tf.Tensor]: Tensors of spatial and temporal points.\n",
    "        \"\"\"\n",
    "        space_coords = []\n",
    "        for (x_min, x_max) in self.x_domains:\n",
    "            coord = np.random.uniform(x_min, x_max, size=(N, 1))\n",
    "            space_coords.append(coord)\n",
    "        space_coords_stacked = np.hstack(space_coords) if space_coords else np.zeros((N, 0))\n",
    "\n",
    "        t_min, t_max = self.t_domain\n",
    "        time_coords = np.random.uniform(t_min, t_max, size=(N, 1))\n",
    "\n",
    "        return tf.convert_to_tensor(space_coords_stacked, dtype=tf.float32), tf.convert_to_tensor(time_coords, dtype=tf.float32)\n",
    "\n",
    "    def train(self, model: tf.keras.Model, epochs: int, N: int, lr: float) -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "        \"\"\"\n",
    "        Train the neural network model on the PDE system.\n",
    "\n",
    "        Parameters:\n",
    "        model (tf.keras.Model): The neural network model.\n",
    "        epochs (int): Number of training epochs.\n",
    "        N (int): Number of points to generate for each epoch.\n",
    "        lr (float): Learning rate for the optimizer.\n",
    "\n",
    "        Returns:\n",
    "        Tuple[np.ndarray, np.ndarray, float]: Loss history, component losses history, and elapsed training time.\n",
    "        \"\"\"\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        loss_history = np.zeros(epochs)\n",
    "        component_losses_history = np.zeros((epochs, self.num_components))\n",
    "        start_time = time.time()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            x_points, t_points = self.generate_points(N)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                overall_loss, component_losses = self.compute_loss(model, t_points, x_points)\n",
    "                overall_loss = tf.cast(overall_loss, dtype=tf.float32)\n",
    "            gradients = tape.gradient(overall_loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "            loss_history[epoch] = overall_loss.numpy()\n",
    "            component_losses_history[epoch, :] = [loss.numpy() for loss in component_losses]\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}: Loss = {overall_loss.numpy()}\")\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        return loss_history, component_losses_history, elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Network Types\n",
    "class UnifiedPINN(tf.keras.Model):\n",
    "    def __init__(self, output_dim: int, architecture: Tuple[int, ...] = (256, 256), activation: str ='tanh'):\n",
    "        \"\"\"\n",
    "        Initialize the UnifiedPINN model.\n",
    "\n",
    "        Parameters:\n",
    "        output_dim (int): The dimension of the output layer.\n",
    "        architecture (Tuple[int, ...]): A tuple defining the number of units in each hidden layer.\n",
    "        activation (str): The activation function to use in the hidden layers.\n",
    "        \"\"\"\n",
    "        super(UnifiedPINN, self).__init__()\n",
    "        # Create hidden layers based on the architecture\n",
    "        self.hidden_layers = [\n",
    "            tf.keras.layers.Dense(units, activation=activation)\n",
    "            for units in architecture\n",
    "        ]\n",
    "        # Create the output layer\n",
    "        self.output_layer = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, x_points: tf.Tensor, t_points: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the UnifiedPINN model.\n",
    "\n",
    "        Parameters:\n",
    "        x_points (tf.Tensor): Tensor of spatial points.\n",
    "        t_points (tf.Tensor): Tensor of temporal points.\n",
    "\n",
    "        Returns:\n",
    "        tf.Tensor: The output of the model.\n",
    "        \"\"\"\n",
    "        # Combine temporal and spatial inputs\n",
    "        combined = tf.concat([t_points, x_points], axis=1)\n",
    "        out = combined\n",
    "        # Pass through hidden layers\n",
    "        for layer in self.hidden_layers:\n",
    "            out = layer(out)\n",
    "        # Pass through the output layer\n",
    "        return self.output_layer(out)\n",
    "\n",
    "class ModularPINNs(tf.keras.Model):\n",
    "    def __init__(self, num_components: int, architectures: List[Tuple[int, ...]] = None, activation: str = 'tanh'):\n",
    "        \"\"\"\n",
    "        Initialize the ModularPINNs model.\n",
    "\n",
    "        Parameters:\n",
    "        num_components (int): Number of components in the PDE system.\n",
    "        architectures (List[Tuple[int, ...]]): List of architectures for each component model.\n",
    "        activation (str): The activation function to use in the hidden layers.\n",
    "        \"\"\"\n",
    "        super(ModularPINNs, self).__init__()\n",
    "        self.num_components = num_components\n",
    "\n",
    "        # Set default architectures if none are provided\n",
    "        if architectures is None:\n",
    "            architectures = [(256, 256)] * num_components\n",
    "\n",
    "        # Ensure the number of architectures matches the number of components\n",
    "        assert len(architectures) == num_components, (\n",
    "            \"The number of architectures must match the number of components.\"\n",
    "        )\n",
    "\n",
    "        # Create a list of UnifiedPINN models for each component\n",
    "        self.models = [\n",
    "            UnifiedPINN(output_dim=1, architecture=arch, activation=activation)\n",
    "            for arch in architectures\n",
    "        ]\n",
    "\n",
    "    def call(self, x_points: tf.Tensor, t_points: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the ModularPINNs model.\n",
    "\n",
    "        Parameters:\n",
    "        x_points (tf.Tensor): Tensor of spatial points.\n",
    "        t_points (tf.Tensor): Tensor of temporal points.\n",
    "\n",
    "        Returns:\n",
    "        tf.Tensor: The concatenated output of all component models.\n",
    "        \"\"\"\n",
    "        outputs = []\n",
    "        # Pass through each component model and collect outputs\n",
    "        for model in self.models:\n",
    "            out = model(x_points, t_points)\n",
    "            outputs.append(out)\n",
    "        # Concatenate outputs from all component models\n",
    "        return tf.concat(outputs, axis=1)\n",
    "\n",
    "    @property\n",
    "    def trainable_weights(self) -> List[tf.Variable]:\n",
    "        \"\"\"\n",
    "        Get the trainable weights of the ModularPINNs model.\n",
    "\n",
    "        Returns:\n",
    "        List[tf.Variable]: List of trainable weights from all component models.\n",
    "        \"\"\"\n",
    "        return [weight for model in self.models for weight in model.trainable_weights]\n",
    "\n",
    "class SemiModularElasticWavePINN(tf.keras.Model):\n",
    "    pass # Placeholder for Semi-Modular Elastic Wave PINN defined later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Comparison Class\n",
    "class UnifiedVsModularComparison:\n",
    "    def __init__(self, system: PDESystem):\n",
    "        self.system = system\n",
    "\n",
    "    def train_unified_and_modular(self, epochs: int, N: int, lr: float, unified_architecture: Tuple[int, ...] = (256, 256), modular_architectures: List[Tuple[int, ...]] = None, activation: str = 'tanh', train_scaled=False, path=\"\") -> Tuple[UnifiedPINN, ModularPINNs]:\n",
    "        unified_model = UnifiedPINN(output_dim=self.system.num_components, architecture=unified_architecture, activation=activation)\n",
    "\n",
    "        if modular_architectures is None:\n",
    "            modular_architectures = [unified_architecture] * self.system.num_components\n",
    "        modular_model = ModularPINNs(num_components=self.system.num_components, architectures=modular_architectures, activation=activation)\n",
    "\n",
    "        unified_loss_history, unified_component_losses_history, unified_time = self.system.train(unified_model, epochs, N, lr)\n",
    "        modular_loss_history, modular_component_losses_history, modular_time = self.system.train(modular_model, epochs, N, lr)\n",
    "        if train_scaled:\n",
    "            arch = self.find_modular_architecture(unified_architecture, self.system.num_components)\n",
    "            print(f\"Modular Scaled Architecture: {arch}\")\n",
    "            scaled_modular_model = ModularPINNs(num_components=self.system.num_components, architectures=([arch]*self.system.num_components), activation=activation)\n",
    "            scaled_modular_loss_history, scaled_modular_component_losses_history, scaled_modular_time = self.system.train(scaled_modular_model, epochs, N, lr)\n",
    "\n",
    "        x_val_points, t_val_points = self.generate_validation_points()\n",
    "        for model in [unified_model, modular_model]:\n",
    "            final_overall_loss, _ = self.system.compute_loss(model, t_val_points, x_val_points)\n",
    "            print(f\"Final Loss for {model}: {final_overall_loss.numpy()}\")\n",
    "        if train_scaled:\n",
    "            for model in [scaled_modular_model]:\n",
    "                final_overall_loss, _ = self.system.compute_loss(model, t_val_points, x_val_points)\n",
    "                print(f\"Final Loss for {model}: {final_overall_loss.numpy()}\")\n",
    "\n",
    "        if not train_scaled:\n",
    "            self.compare_overall_loss(unified_loss_history, modular_loss_history)\n",
    "            self.compare_component_losses(unified_component_losses_history, modular_component_losses_history)\n",
    "        if train_scaled:\n",
    "            self.compare_overall_loss(unified_loss_history, modular_loss_history, scaled_modular_loss=scaled_modular_loss_history)\n",
    "            self.compare_component_losses(unified_component_losses_history, modular_component_losses_history, scaled_modular_loss=scaled_modular_component_losses_history)\n",
    "\n",
    "        unified_model.summary()\n",
    "        modular_model.summary()\n",
    "        if train_scaled:\n",
    "            scaled_modular_model.summary()\n",
    "\n",
    "        self.compute_convergence_epochs(unified_loss_history)\n",
    "        self.compute_convergence_epochs(modular_loss_history)\n",
    "        if train_scaled:\n",
    "            self.compute_convergence_epochs(scaled_modular_loss_history)\n",
    "\n",
    "        print(f\"Unified PINN training time: {unified_time:.2f} seconds\")\n",
    "        print(f\"Modular PINN training time: {modular_time:.2f} seconds\")\n",
    "        print(f\"Relative training time: {((modular_time-unified_time)/unified_time):.2f}\")\n",
    "        if train_scaled:\n",
    "            print(f\"Scaled Modular PINN training time: {scaled_modular_time:.2f} seconds\")\n",
    "            print(f\"Relative training time (to scaled): {((scaled_modular_time-unified_time)/unified_time):.2f}\")\n",
    "\n",
    "        if path == \"\":\n",
    "            path = self.system.__class__.__name__\n",
    "        else:\n",
    "            path = self.system.__class__.__name__ + \"_\" + path\n",
    "        self.save_model(unified_model, f\"{path}_unified\", modular=False)\n",
    "        self.save_model(modular_model, f\"{path}_modular\", modular=True)\n",
    "        if train_scaled:\n",
    "            self.save_model(scaled_modular_model, f\"{path}_scaled_modular_model\", modular=True)\n",
    "\n",
    "        if train_scaled:\n",
    "            return unified_model, modular_model, scaled_modular_model\n",
    "        return unified_model, modular_model\n",
    "\n",
    "    @staticmethod\n",
    "    def compare_component_losses(unified_component_losses, modular_component_losses, semi_modular_loss=None, scaled_modular_loss=None):\n",
    "      assert unified_component_losses.shape[1] == modular_component_losses.shape[1], \\\n",
    "          \"Number of components must match between Unified and Modular PINNs.\"\n",
    "\n",
    "      num_components = unified_component_losses.shape[1]\n",
    "      fig, axes = plt.subplots(1, num_components, figsize=(5 * num_components, 6), sharey=True)\n",
    "\n",
    "      if num_components == 1:\n",
    "          axes = [axes]\n",
    "\n",
    "      for i in range(num_components):\n",
    "          axes[i].plot(unified_component_losses[:, i], label=f\"Unified Component {i + 1}\", color=\"blue\")\n",
    "          axes[i].plot(modular_component_losses[:, i], label=f\"Modular Component {i + 1}\", color=\"green\")\n",
    "          if semi_modular_loss is not None:\n",
    "              axes[i].plot(semi_modular_loss[:, i], label=f\"Semi-Modular Component {i + 1}\", color=\"red\")\n",
    "          if scaled_modular_loss is not None:\n",
    "              axes[i].plot(scaled_modular_loss[:, i], label=f\"Scaled Modular Component {i + 1}\", color=\"purple\")\n",
    "          axes[i].set_xlabel(\"Epochs\")\n",
    "          axes[i].set_yscale(\"log\")\n",
    "          axes[i].set_title(f\"Component {i + 1} Loss\")\n",
    "          axes[i].legend()\n",
    "          axes[i].grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "      plt.tight_layout()\n",
    "      plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def compare_overall_loss(unified_loss, modular_loss, semi_modular_loss=None, scaled_modular_loss=None):\n",
    "        assert len(unified_loss) == len(modular_loss), \\\n",
    "            \"Unified and Modular loss histories must have the same length.\"\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(unified_loss, label=\"Unified Overall Loss\", color=\"blue\")\n",
    "        plt.plot(modular_loss, label=\"Modular Overall Loss\", color=\"green\")\n",
    "        if semi_modular_loss is not None:\n",
    "            plt.plot(semi_modular_loss, label=\"Semi-Modular Overall Loss\", color=\"red\")\n",
    "        if scaled_modular_loss is not None:\n",
    "            plt.plot(scaled_modular_loss, label=\"Scaled Modular Overall Loss\", color=\"purple\")\n",
    "\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Log Loss\")\n",
    "        plt.yscale(\"log\")\n",
    "        if semi_modular_loss is not None:\n",
    "            plt.title(\"Unified vs Modular vs Semi-Modular Overall Loss\")\n",
    "        else:\n",
    "            plt.title(\"Unified vs Modular Overall Loss\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_convergence_epochs(loss_history, thresholds: List[float] = [1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8]):\n",
    "        convergence_epochs = {}\n",
    "        for threshold in thresholds:\n",
    "            epoch = next((i for i, loss in enumerate(loss_history) if loss < threshold), None)\n",
    "            convergence_epochs[threshold] = epoch\n",
    "        print(f\"Convergence epochs for different thresholds: {convergence_epochs}\")\n",
    "        return convergence_epochs\n",
    "\n",
    "    @staticmethod\n",
    "    def find_modular_architecture(unified_arch, num_components, max_iter=2000, tolerance=0.05):\n",
    "        def compute_parameters(layers):\n",
    "            weights = sum(layers[i] * layers[i + 1] for i in range(len(layers) - 1))\n",
    "            biases = sum(layers)\n",
    "            return weights + biases\n",
    "\n",
    "        unified_params = compute_parameters(unified_arch)\n",
    "        print(f\"Unified Network Parameters: {unified_params}\")\n",
    "\n",
    "        # Preserve the proportion of neurons across layers\n",
    "        total_neurons = sum(unified_arch)\n",
    "        layer_proportions = [layer / total_neurons for layer in unified_arch]\n",
    "\n",
    "        modular_arch = [max(1, int(neurons * sum(unified_arch) // num_components)) for neurons in unified_arch]\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            modular_params = num_components * compute_parameters(modular_arch)\n",
    "\n",
    "            if abs(modular_params - unified_params) / unified_params < tolerance:\n",
    "                print(f\"Modular Network Parameters: {modular_params}\")\n",
    "                return modular_arch\n",
    "\n",
    "            # Scale each layer proportionally to match parameters\n",
    "            scale_factor = np.sqrt(unified_params / modular_params)\n",
    "            modular_arch = [max(1, int(layer * scale_factor)) for layer in modular_arch]\n",
    "\n",
    "        raise ValueError(\"Failed to match modular architecture within tolerance.\")\n",
    "\n",
    "    def generate_validation_points(self, N: int = 1000):\n",
    "        spatial_dims = len(self.system.x_domains)\n",
    "        spatial_points_per_dim = int(np.ceil(N ** (1 / (spatial_dims + 1))))\n",
    "        temporal_points = int(np.ceil(N / (spatial_points_per_dim ** spatial_dims)))\n",
    "\n",
    "        spatial_grids = [\n",
    "            np.linspace(domain[0], domain[1], spatial_points_per_dim)\n",
    "            for domain in self.system.x_domains\n",
    "        ]\n",
    "        spatial_mesh = np.meshgrid(*spatial_grids)\n",
    "        spatial_coords = np.stack([mesh.flatten() for mesh in spatial_mesh], axis=1)\n",
    "\n",
    "        t_min, t_max = self.system.t_domain\n",
    "        temporal_coords = np.linspace(t_min, t_max, temporal_points).reshape(-1, 1)\n",
    "\n",
    "        spatial_repeats = temporal_coords.shape[0]\n",
    "        spatial_expanded = np.repeat(spatial_coords, spatial_repeats, axis=0)\n",
    "        temporal_tiled = np.tile(temporal_coords, (spatial_coords.shape[0], 1))\n",
    "\n",
    "        x_points = tf.convert_to_tensor(spatial_expanded, dtype=tf.float32)\n",
    "        t_points = tf.convert_to_tensor(temporal_tiled, dtype=tf.float32)\n",
    "\n",
    "        return x_points, t_points\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(model: tf.keras.Model, path: str, modular: bool = False):\n",
    "        if modular:\n",
    "            for i, sub_model in enumerate(model.models):\n",
    "                sub_model.save_weights(f\"{path}_model_{i}.weights.h5\")\n",
    "        else:\n",
    "            model.save_weights(f\"{path}.weights.h5\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(path: str, num_components: int = None, modular: bool = False, architectures: List[Tuple[int, ...]]  = [(256, 256)], activation: str = 'tanh'):\n",
    "        if modular:\n",
    "            if num_components is None:\n",
    "                raise ValueError(\"num_components must be specified for ModularPINNs.\")\n",
    "            modular_model = ModularPINNs(num_components=num_components,\n",
    "                                         architectures=architectures,\n",
    "                                         activation=activation)\n",
    "            for i, sub_model in enumerate(modular_model.models):\n",
    "                sub_model.load_weights(f\"{path}_model_{i}.weights.h5\")\n",
    "            return modular_model\n",
    "        else:\n",
    "            unified_model = UnifiedPINN(output_dim=1, architecture=architectures[0], activation=activation)\n",
    "            unified_model.load_weights(f\"{path}.weights.h5\")\n",
    "            return unified_model\n",
    "\n",
    "    def plot_neural_network_activation(self, model, x_points, t_points, save_path=\"network_visualization\"):\n",
    "        \"\"\"Visualize neuron activations for all output components.\"\"\"\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        combined_input = tf.concat([t_points, x_points], axis=1)\n",
    "\n",
    "        if isinstance(model, SemiModularElasticWavePINN):\n",
    "            # Handle semi-modular case\n",
    "            self.plot_single_network_activation(\n",
    "                model.displacement_hidden_layers, model.displacement_output_layer,\n",
    "                combined_input, save_path, label=\"Displacement\"\n",
    "            )\n",
    "            self.plot_single_network_activation(\n",
    "                model.stress_hidden_layers, model.stress_output_layer,\n",
    "                combined_input, save_path, label=\"Stress\"\n",
    "            )\n",
    "        elif isinstance(model, ModularPINNs):\n",
    "            # Handle modular networks\n",
    "            for i, sub_model in enumerate(model.models):\n",
    "                self.plot_single_network_activation(\n",
    "                    sub_model.hidden_layers, sub_model.output_layer,\n",
    "                    combined_input, save_path, label=f\"Component_{i + 1}\"\n",
    "                )\n",
    "        else:\n",
    "            # Handle unified network\n",
    "            self.plot_single_network_activation(\n",
    "                model.hidden_layers, model.output_layer,\n",
    "                combined_input, save_path, label=\"Unified\"\n",
    "            )\n",
    "\n",
    "    def plot_single_network_activation(self, hidden_layers, output_layer, combined_input, save_path, label):\n",
    "        \"\"\"Helper function to visualize activations for a single network.\"\"\"\n",
    "        activations = []\n",
    "        out = combined_input\n",
    "\n",
    "        # Collect activations layer by layer\n",
    "        for layer in hidden_layers:\n",
    "            out = layer(out)\n",
    "            activations.append(out.numpy())\n",
    "        activations.append(output_layer(out).numpy())\n",
    "\n",
    "        # Plot the network structure with activations\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        max_neurons = max(layer.shape[1] for layer in activations)\n",
    "\n",
    "        for layer_idx, layer_activations in enumerate(activations):\n",
    "            num_neurons = layer_activations.shape[1]\n",
    "            x_positions = np.linspace(-max_neurons / 2, max_neurons / 2, num_neurons)\n",
    "            y_position = -layer_idx\n",
    "\n",
    "            # Normalize activations to color scale\n",
    "            norm_activations = (layer_activations - layer_activations.min()) / (\n",
    "                layer_activations.max() - layer_activations.min()\n",
    "            )\n",
    "\n",
    "            # Plot neurons\n",
    "            scatter = plt.scatter(\n",
    "                x_positions,\n",
    "                [y_position] * num_neurons,\n",
    "                s=200,\n",
    "                c=norm_activations.mean(axis=0),\n",
    "                cmap=\"viridis\",\n",
    "                edgecolor=\"k\",\n",
    "                zorder=2,\n",
    "            )\n",
    "\n",
    "            # Connect layers\n",
    "            if layer_idx < len(activations) - 1:\n",
    "                next_layer_activations = activations[layer_idx + 1]\n",
    "                next_x_positions = np.linspace(\n",
    "                    -max_neurons / 2, max_neurons / 2, next_layer_activations.shape[1]\n",
    "                )\n",
    "                for i, x_pos in enumerate(x_positions):\n",
    "                    for j, next_x_pos in enumerate(next_x_positions):\n",
    "                        plt.plot(\n",
    "                            [x_pos, next_x_pos], [y_position, y_position - 1], \"gray\", alpha=0.3, zorder=1\n",
    "                        )\n",
    "\n",
    "        cbar = plt.colorbar(scatter, pad=0.02)\n",
    "        cbar.set_label(\"Activation Intensity\")\n",
    "\n",
    "        plt.title(f\"Neuron Activations ({label})\")\n",
    "        plt.xlabel(\"Neuron Index\")\n",
    "        plt.ylabel(\"Layer\")\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid(False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{save_path}/{label}_activations.png\")\n",
    "        plt.close()\n",
    "\n",
    "    def collect_activations(self, model, x_points, t_points):\n",
    "        combined_input = tf.concat([t_points, x_points], axis=1)\n",
    "        activations = {}\n",
    "\n",
    "        if isinstance(model, SemiModularElasticWavePINN):\n",
    "            # Collect activations for displacement sub-network\n",
    "            displacement_activations = []\n",
    "            out_displacement = combined_input\n",
    "            for layer in model.displacement_hidden_layers:\n",
    "                out_displacement = layer(out_displacement)\n",
    "                displacement_activations.append(out_displacement.numpy())\n",
    "            displacement_activations.append(model.displacement_output_layer(out_displacement).numpy())\n",
    "\n",
    "            # Collect activations for stress sub-network\n",
    "            stress_activations = []\n",
    "            out_stress = combined_input\n",
    "            for layer in model.stress_hidden_layers:\n",
    "                out_stress = layer(out_stress)\n",
    "                stress_activations.append(out_stress.numpy())\n",
    "            stress_activations.append(model.stress_output_layer(out_stress).numpy())\n",
    "\n",
    "            # Store activations separately\n",
    "            activations['displacement'] = displacement_activations\n",
    "            activations['stress'] = stress_activations\n",
    "\n",
    "        elif isinstance(model, ModularPINNs):\n",
    "            # Collect activations for modular sub-networks\n",
    "            for i, sub_model in enumerate(model.models):\n",
    "                modular_activations = []\n",
    "                out = combined_input\n",
    "                for layer in sub_model.hidden_layers:\n",
    "                    out = layer(out)\n",
    "                    modular_activations.append(out.numpy())\n",
    "                modular_activations.append(sub_model.output_layer(out).numpy())\n",
    "                activations[f'component_{i + 1}'] = modular_activations\n",
    "\n",
    "        else:  # Unified PINN\n",
    "            unified_activations = []\n",
    "            out = combined_input\n",
    "            for layer in model.hidden_layers:\n",
    "                out = layer(out)\n",
    "                unified_activations.append(out.numpy())\n",
    "            unified_activations.append(model.output_layer(out).numpy())\n",
    "            activations['unified'] = unified_activations\n",
    "\n",
    "        return activations\n",
    "\n",
    "\n",
    "    def compute_neuron_activation_overlap(self, activations, threshold=0.05):\n",
    "        \"\"\"\n",
    "        Compute overlap and correlation metrics between neuron activations\n",
    "        for different layers (layer-wise) and across components (component-wise)\n",
    "        across unified, modular, and semi-modular architectures.\n",
    "        \"\"\"\n",
    "        layer_overlap_metrics = []\n",
    "        layer_correlation_metrics = []\n",
    "        component_overlap_metrics = []\n",
    "        component_correlation_metrics = []\n",
    "\n",
    "        def normalize_activations(act1, act2):\n",
    "            min_dim = min(act1.shape[1], act2.shape[1])\n",
    "            act1 = act1[:, :min_dim]\n",
    "            act2 = act2[:, :min_dim]\n",
    "            return act1, act2\n",
    "\n",
    "        def calculate_overlap(activated_i, activated_j):\n",
    "            union = np.sum(activated_i | activated_j)\n",
    "            if union == 0:\n",
    "                return 0  # Avoid division by zero\n",
    "            return np.sum(activated_i & activated_j) / union\n",
    "\n",
    "        # Unified Network\n",
    "        if 'unified' in activations:\n",
    "            unified_activations = activations['unified']\n",
    "            num_layers = len(unified_activations)\n",
    "\n",
    "            # Layer-wise analysis\n",
    "            for i in range(num_layers - 1):\n",
    "                layer_activations_i = unified_activations[i]\n",
    "                layer_activations_j = unified_activations[i + 1]\n",
    "\n",
    "                if layer_activations_i.shape[1] != layer_activations_j.shape[1]:\n",
    "                    layer_activations_i, layer_activations_j = normalize_activations(layer_activations_i, layer_activations_j)\n",
    "\n",
    "                activated_i = np.abs(layer_activations_i) > threshold\n",
    "                activated_j = np.abs(layer_activations_j) > threshold\n",
    "                overlap = calculate_overlap(activated_i, activated_j)\n",
    "                correlation = np.corrcoef(layer_activations_i.flatten(), layer_activations_j.flatten())[0, 1]\n",
    "\n",
    "                layer_overlap_metrics.append(overlap)\n",
    "                layer_correlation_metrics.append(correlation)\n",
    "\n",
    "            # Component-wise analysis\n",
    "            for i in range(self.system.num_components):\n",
    "                for j in range(i + 1, self.system.num_components):\n",
    "                    component_activations_i = unified_activations[i]\n",
    "                    component_activations_j = unified_activations[j]\n",
    "\n",
    "                    if component_activations_i.shape[1] != component_activations_j.shape[1]:\n",
    "                        component_activations_i, component_activations_j = normalize_activations(component_activations_i, component_activations_j)\n",
    "\n",
    "                    activated_i = np.abs(component_activations_i) > threshold\n",
    "                    activated_j = np.abs(component_activations_j) > threshold\n",
    "                    overlap = calculate_overlap(activated_i, activated_j)\n",
    "                    correlation = np.corrcoef(component_activations_i.flatten(), component_activations_j.flatten())[0, 1]\n",
    "\n",
    "                    component_overlap_metrics.append(overlap)\n",
    "                    component_correlation_metrics.append(correlation)\n",
    "\n",
    "        # Modular Network\n",
    "        elif isinstance(activations, dict) and all(key.startswith('component') for key in activations.keys()):\n",
    "            keys = list(activations.keys())\n",
    "\n",
    "            # Layer-wise analysis for each sub-network\n",
    "            for key in keys:\n",
    "                component_activations = activations[key]\n",
    "                num_layers = len(component_activations)\n",
    "\n",
    "                for i in range(num_layers - 1):\n",
    "                    layer_activations_i = component_activations[i]\n",
    "                    layer_activations_j = component_activations[i + 1]\n",
    "\n",
    "                    if layer_activations_i.shape[1] != layer_activations_j.shape[1]:\n",
    "                        layer_activations_i, layer_activations_j = normalize_activations(layer_activations_i, layer_activations_j)\n",
    "\n",
    "                    activated_i = np.abs(layer_activations_i) > threshold\n",
    "                    activated_j = np.abs(layer_activations_j) > threshold\n",
    "                    overlap = calculate_overlap(activated_i, activated_j)\n",
    "                    correlation = np.corrcoef(layer_activations_i.flatten(), layer_activations_j.flatten())[0, 1]\n",
    "\n",
    "                    layer_overlap_metrics.append(overlap)\n",
    "                    layer_correlation_metrics.append(correlation)\n",
    "\n",
    "            # Component-wise analysis\n",
    "            for i in range(len(keys)):\n",
    "                for j in range(i + 1, len(keys)):\n",
    "                    component_activations_i = activations[keys[i]]\n",
    "                    component_activations_j = activations[keys[j]]\n",
    "\n",
    "                    for layer_idx in range(len(component_activations_i)):\n",
    "                        layer_activations_i = component_activations_i[layer_idx]\n",
    "                        layer_activations_j = component_activations_j[layer_idx]\n",
    "\n",
    "                        if layer_activations_i.shape[1] != layer_activations_j.shape[1]:\n",
    "                            layer_activations_i, layer_activations_j = normalize_activations(layer_activations_i, layer_activations_j)\n",
    "\n",
    "                        activated_i = np.abs(layer_activations_i) > threshold\n",
    "                        activated_j = np.abs(layer_activations_j) > threshold\n",
    "                        overlap = calculate_overlap(activated_i, activated_j)\n",
    "                        correlation = np.corrcoef(layer_activations_i.flatten(), layer_activations_j.flatten())[0, 1]\n",
    "\n",
    "                        component_overlap_metrics.append(overlap)\n",
    "                        component_correlation_metrics.append(correlation)\n",
    "\n",
    "        # Return all metrics\n",
    "        avg_layer_overlap = np.mean(layer_overlap_metrics) if layer_overlap_metrics else 0\n",
    "        avg_layer_correlation = np.mean(layer_correlation_metrics) if layer_correlation_metrics else 0\n",
    "        avg_component_overlap = np.mean(component_overlap_metrics) if component_overlap_metrics else 0\n",
    "        avg_component_correlation = np.mean(component_correlation_metrics) if component_correlation_metrics else 0\n",
    "\n",
    "        return (\n",
    "            layer_overlap_metrics,\n",
    "            layer_correlation_metrics,\n",
    "            avg_layer_overlap,\n",
    "            avg_layer_correlation,\n",
    "            component_overlap_metrics,\n",
    "            component_correlation_metrics,\n",
    "            avg_component_overlap,\n",
    "            avg_component_correlation,\n",
    "        )\n",
    "\n",
    "    def analyze_interpretability(self, models, save_path=\"interpretability_analysis\"):\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        # Generate test points\n",
    "        x_points, t_points = self.generate_validation_points()\n",
    "\n",
    "        for model, name in models:\n",
    "            print(f\"Analyzing {name}...\")\n",
    "\n",
    "            try:\n",
    "                # Neuron Activation Visualization\n",
    "                activations = self.collect_activations(model, x_points, t_points)\n",
    "                (\n",
    "                    layer_overlaps, layer_correlations, avg_layer_overlap, avg_layer_correlation,\n",
    "                    component_overlaps, component_correlations, avg_component_overlap, avg_component_correlation\n",
    "                ) = self.compute_neuron_activation_overlap(activations)\n",
    "\n",
    "                print(f\"{name} - Average Layer-wise Overlap: {avg_layer_overlap:.4f}, Average Layer-wise Correlation: {avg_layer_correlation:.4f}\")\n",
    "                print(f\"{name} - Average Component-wise Overlap: {avg_component_overlap:.4f}, Average Component-wise Correlation: {avg_component_correlation:.4f}\")\n",
    "\n",
    "                # Save plots for neuron activations\n",
    "                self.plot_neural_network_activation(model, x_points, t_points, save_path=f\"{save_path}/{name}_activation\")\n",
    "\n",
    "                # Neuron sparsity\n",
    "                self.analyze_network_sparsity(model, x_points, t_points)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Interpretability analysis failed for {name}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "\n",
    "    def compute_activation_sparsity(self, activations, threshold=0.001):\n",
    "        \"\"\"\n",
    "        Computes activation sparsity for each hidden layer:\n",
    "        - Fraction of neurons that remain inactive (activation < threshold).\n",
    "        - Returns sparsity per layer and overall average sparsity.\n",
    "        \"\"\"\n",
    "        sparsity_metrics = []\n",
    "\n",
    "        # Exclude the output layer (last layer)\n",
    "        hidden_layers = activations[:-1]\n",
    "\n",
    "        for layer_activations in hidden_layers:\n",
    "            inactive_neurons = np.mean(np.abs(layer_activations) < threshold, axis=0)\n",
    "            layer_sparsity = np.mean(inactive_neurons)\n",
    "            sparsity_metrics.append(layer_sparsity)\n",
    "\n",
    "        return sparsity_metrics, np.mean(sparsity_metrics)\n",
    "\n",
    "\n",
    "    def analyze_network_sparsity(self, model, x_points, t_points):\n",
    "        \"\"\"\n",
    "        Computes and plots activation sparsity for both\n",
    "        unified and modular networks, **excluding the output layer**.\n",
    "        \"\"\"\n",
    "        activations = self.collect_activations(model, x_points, t_points)\n",
    "\n",
    "        sparsity_results = {}\n",
    "\n",
    "        if isinstance(model, ModularPINNs):\n",
    "            # For Modular Networks, compute sparsity per component\n",
    "            for i, (component, component_activations) in enumerate(activations.items()):\n",
    "                sparsity_metrics, avg_sparsity = self.compute_activation_sparsity(component_activations)\n",
    "\n",
    "                sparsity_results[f'Component {i+1}'] = (sparsity_metrics, avg_sparsity)\n",
    "        elif isinstance(model, SemiModularElasticWavePINN):\n",
    "            # Compute sparsity for displacement and stress subnetworks separately\n",
    "            sparsity_displacement, avg_sparsity_displacement = self.compute_activation_sparsity(activations['displacement'])\n",
    "            sparsity_stress, avg_sparsity_stress = self.compute_activation_sparsity(activations['stress'])\n",
    "\n",
    "            sparsity_results['Displacement'] = (sparsity_displacement, avg_sparsity_displacement)\n",
    "            sparsity_results['Stress'] = (sparsity_stress, avg_sparsity_stress)\n",
    "        else:\n",
    "            # For Unified Networks, compute sparsity across all layers\n",
    "            sparsity_metrics, avg_sparsity = self.compute_activation_sparsity(activations['unified'])\n",
    "\n",
    "            sparsity_results['Unified'] = (sparsity_metrics, avg_sparsity)\n",
    "\n",
    "        self.plot_sparsity(sparsity_results, activations)\n",
    "\n",
    "    def compute_sparsity_cdf(self, activations_dict):\n",
    "        \"\"\"\n",
    "        Computes the cumulative fraction of neurons that remain inactive (below threshold).\n",
    "        This creates a sparsity CDF across different activation thresholds for unified and modular networks.\n",
    "        \"\"\"\n",
    "\n",
    "        # Expand thresholds by adding intermediate values\n",
    "        base_thresholds = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "        fine_thresholds = []\n",
    "\n",
    "        for i in range(len(base_thresholds) - 1):\n",
    "            start, end = base_thresholds[i], base_thresholds[i + 1]\n",
    "            fine_thresholds.extend(np.linspace(start, end, 10, endpoint=False))  # Add 9 intermediate points\n",
    "\n",
    "        fine_thresholds.append(1e-1)  # Ensure last value is included\n",
    "        thresholds = sorted(fine_thresholds)  # Sorted for clarity\n",
    "\n",
    "        sparsity_cdf_results = {}\n",
    "\n",
    "        for key, activations in activations_dict.items():\n",
    "            if not isinstance(activations, list) or len(activations) == 0:\n",
    "                print(f\"Skipping CDF for {key}: No activations available.\")\n",
    "                continue\n",
    "\n",
    "            sparsity_cdf = []\n",
    "            hidden_layers = activations[:-1]  # Exclude output layer\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                inactive_counts = []\n",
    "                for layer_activations in hidden_layers:\n",
    "                    if isinstance(layer_activations, np.ndarray) and layer_activations.shape[0] > 0:\n",
    "                        inactive_neurons = np.mean(np.abs(layer_activations) < threshold, axis=0)\n",
    "                        inactive_counts.append(np.mean(inactive_neurons))\n",
    "                    else:\n",
    "                        inactive_counts.append(0)  # Default to 0 if no activations exist\n",
    "                sparsity_cdf.append(np.mean(inactive_counts))  # Average across layers\n",
    "\n",
    "            sparsity_cdf_results[key] = (thresholds, sparsity_cdf)\n",
    "\n",
    "        return sparsity_cdf_results\n",
    "\n",
    "    def plot_sparsity(self, sparsity_results, activations):\n",
    "        \"\"\"\n",
    "        Plots activation sparsity and a CDF of sparsity for unified and modular networks.\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(18, 5))\n",
    "\n",
    "        for component, (sparsity_metrics, avg_sparsity) in sparsity_results.items():\n",
    "            layer_indices = list(range(len(sparsity_metrics)))  # Ensure integer indices\n",
    "            axes[0].plot(layer_indices, sparsity_metrics, label=f\"{component} (Avg: {avg_sparsity:.4f})\")\n",
    "\n",
    "        axes[0].set_xlabel(\"Layer Index\")\n",
    "        axes[0].set_ylabel(\"Activation Sparsity\")\n",
    "        axes[0].set_title(\"Activation Sparsity Across Hidden Layers\")\n",
    "        axes[0].legend()\n",
    "        axes[0].grid()\n",
    "        axes[0].set_xticks(layer_indices)  # Set ticks to integer layer indices\n",
    "\n",
    "        sparsity_cdf_results = self.compute_sparsity_cdf(activations)\n",
    "\n",
    "        for component, (thresholds, sparsity_cdf) in sparsity_cdf_results.items():\n",
    "            axes[1].plot(thresholds, sparsity_cdf, marker='o', label=f\"{component}\")\n",
    "\n",
    "        axes[1].set_xlabel(\"Activation Threshold\")\n",
    "        axes[1].set_xscale(\"log\")  # Log scale for better visualization\n",
    "        axes[1].set_ylabel(\"Fraction of Neurons Inactive\")\n",
    "        axes[1].set_title(\"Sparsity CDF\")\n",
    "        axes[1].legend()\n",
    "        axes[1].grid()\n",
    "\n",
    "        # Modify x-axis ticks to only show base 1e-x values\n",
    "        base_thresholds = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "        axes[1].set_xticks(base_thresholds)  # Keep major thresholds\n",
    "        axes[1].set_xticklabels([f\"$10^{{-{int(np.log10(x))}}}$\" for x in base_thresholds])  # Use scientific notation\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Reaction Diffusion Systems\n",
    "class ReactionDiffusionSystem(PDESystem):\n",
    "    def __init__(self):\n",
    "        super().__init__(num_components=3, x_domains=[(0, 1)], t_domain=(0, 1))\n",
    "\n",
    "    def residuals(self, model, t_points, x_points):\n",
    "        with tf.GradientTape(persistent=True) as tape2:\n",
    "            tape2.watch([t_points, x_points])\n",
    "\n",
    "            with tf.GradientTape(persistent=True) as tape1:\n",
    "                tape1.watch([t_points, x_points])\n",
    "                u_pred = model(x_points, t_points)\n",
    "                u1, u2, u3 = tf.split(u_pred, num_or_size_splits=3, axis=1)\n",
    "\n",
    "            u1_t = tape1.gradient(u1, t_points)\n",
    "            u2_t = tape1.gradient(u2, t_points)\n",
    "            u3_t = tape1.gradient(u3, t_points)\n",
    "\n",
    "            u1_x = tape1.gradient(u1, x_points)\n",
    "            u2_x = tape1.gradient(u2, x_points)\n",
    "            u3_x = tape1.gradient(u3, x_points)\n",
    "\n",
    "        u1_xx = tape2.gradient(u1_x, x_points)\n",
    "        u2_xx = tape2.gradient(u2_x, x_points)\n",
    "        u3_xx = tape2.gradient(u3_x, x_points)\n",
    "\n",
    "        del tape1, tape2\n",
    "\n",
    "        res1 = u1_t - 0.1 * u1_xx + u1 * u2 - u3\n",
    "        res2 = u2_t - 0.2 * u2_xx + u2 * u3 - u1\n",
    "        res3 = u3_t - 0.3 * u3_xx + u3 * u1 - u2\n",
    "\n",
    "        return [res1, res2, res3]\n",
    "\n",
    "class ReactionDiffusionFirstOrderSystem(PDESystem):\n",
    "    def __init__(self):\n",
    "        super().__init__(num_components=6, x_domains=[(0, 1)], t_domain=(0, 1))\n",
    "\n",
    "    def residuals(self, model, t_points, x_points):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch([t_points, x_points])\n",
    "            u_pred = model(x_points, t_points)\n",
    "            u1, u2, u3, v1, v2, v3 = tf.split(u_pred, num_or_size_splits=6, axis=1)\n",
    "\n",
    "            u1_t = tape.gradient(u1, t_points)\n",
    "            u2_t = tape.gradient(u2, t_points)\n",
    "            u3_t = tape.gradient(u3, t_points)\n",
    "\n",
    "            v1_t = tape.gradient(v1, t_points)\n",
    "            v2_t = tape.gradient(v2, t_points)\n",
    "            v3_t = tape.gradient(v3, t_points)\n",
    "\n",
    "            v1_x = tape.gradient(v1, x_points)\n",
    "            v2_x = tape.gradient(v2, x_points)\n",
    "            v3_x = tape.gradient(v3, x_points)\n",
    "\n",
    "        del tape\n",
    "\n",
    "        res1 = u1_t - 0.1 * v1_x + u1 * u2 - u3\n",
    "        res2 = u2_t - 0.2 * v2_x + u2 * u3 - u1\n",
    "        res3 = u3_t - 0.3 * v3_x + u3 * u1 - u2\n",
    "        res4 = v1_t + v1_x\n",
    "        res5 = v2_t + v2_x\n",
    "        res6 = v3_t + v3_x\n",
    "\n",
    "        return [res1, res2, res3, res4, res5, res6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Elastic Wave Systems\n",
    "class ElasticWaveSystem(PDESystem):\n",
    "    def __init__(self, rho=1.0, C11=1.0, C12=1.0, C22=1.0, C33=1.0):\n",
    "        super().__init__(num_components=5, x_domains=[(0, 1), (0, 1)], t_domain=(0, 1))\n",
    "        self.rho = rho\n",
    "        self.C11 = C11\n",
    "        self.C12 = C12\n",
    "        self.C22 = C22\n",
    "        self.C33 = C33\n",
    "\n",
    "    def residuals(self, model, t_points, x_points):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch([t_points, x_points])\n",
    "\n",
    "            u_pred = model(x_points, t_points)\n",
    "            u_x, u_y, sigma_xx, sigma_yy, sigma_xy = tf.split(u_pred, num_or_size_splits=5, axis=1)\n",
    "\n",
    "            u_x_t = tape.gradient(u_x, t_points)\n",
    "            u_y_t = tape.gradient(u_y, t_points)\n",
    "\n",
    "            u_x_gradients = tape.gradient(u_x, x_points)\n",
    "            u_y_gradients = tape.gradient(u_y, x_points)\n",
    "\n",
    "            u_x_x1 = u_x_gradients[:, 0:1]\n",
    "            u_x_x2 = u_x_gradients[:, 1:2]\n",
    "            u_y_x1 = u_y_gradients[:, 0:1]\n",
    "            u_y_x2 = u_y_gradients[:, 1:2]\n",
    "\n",
    "            sigma_xx = self.C11 * u_x_x1 + self.C12 * u_y_x1\n",
    "            sigma_yy = self.C22 * u_y_x2 + self.C12 * u_x_x2\n",
    "            sigma_xy = self.C33 * (u_x_x2 + u_y_x1)\n",
    "\n",
    "            u_x_tt = tape.gradient(u_x_t, t_points)\n",
    "            u_y_tt = tape.gradient(u_y_t, t_points)\n",
    "\n",
    "            sigma_xx_gradients = tape.gradient(sigma_xx, x_points)\n",
    "            sigma_xy_gradients = tape.gradient(sigma_xy, x_points)\n",
    "            sigma_yy_gradients = tape.gradient(sigma_yy, x_points)\n",
    "\n",
    "        res_u_x = self.rho * u_x_tt - (sigma_xx_gradients[:, 0:1] + sigma_xy_gradients[:, 1:2])\n",
    "        res_u_y = self.rho * u_y_tt - (sigma_xy_gradients[:, 0:1] + sigma_yy_gradients[:, 1:2])\n",
    "\n",
    "        del tape\n",
    "        return [res_u_x, res_u_y, sigma_xx, sigma_yy, sigma_xy]\n",
    "\n",
    "class ElasticWaveSystemStronglyCoupled(ElasticWaveSystem):\n",
    "    def __init__(self):\n",
    "        super().__init__(rho=1.0, C11=1.0, C12=0.9, C22=1.0, C33=0.8)\n",
    "\n",
    "class ElasticWaveSystemDirectionalDominance(ElasticWaveSystem):\n",
    "    def __init__(self):\n",
    "        super().__init__(rho=1.0, C11=2.0, C12=0.5, C22=1.0, C33=0.8)\n",
    "\n",
    "class ElasticWaveSystemAnisotropic(ElasticWaveSystem):\n",
    "    def __init__(self):\n",
    "        super().__init__(rho=1.0, C11=5.0, C12=0.2, C22=1.0, C33=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Reaction-Diffusion System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Experiments\n",
    "comp = UnifiedVsModularComparison(ReactionDiffusionSystem())\n",
    "\n",
    "for (arch, name) in experiments:\n",
    "    uni, mod, scaled = comp.train_unified_and_modular(500, 1000, 1e-3, unified_architecture=arch, train_scaled=True)\n",
    "\n",
    "    models = [\n",
    "        (uni, \"UnifiedPINN\"),\n",
    "        (mod, \"ModularPINN\"),\n",
    "        (scaled, \"ScaledModularPINN\")\n",
    "    ]\n",
    "\n",
    "    comp.analyze_interpretability(models, save_path=f\"ReactionDiffusion_{name}_Interpretability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Data\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "\n",
    "data = {\n",
    "    'Architecture': [(64, 64), (128, 64), (128, 64, 32), (64, 64, 64), (64, 128, 128, 64),\n",
    "                     (256, 256), (64, 64, 64, 64), (512, 256, 128, 64), (256, 256, 256, 256)],\n",
    "    'Total_Params': [4547, 8835, 10819, 8707, 33475, 67331, 12867, 174211, 198915],\n",
    "    'Unified_Final_Loss': [4.93e-6, 1.21e-6, 1.37e-6, 1.55e-6, 5.76e-7, 9.25e-8, 1.46e-6, 5.22e-8, 1.82e-8],\n",
    "    'Unified_Layerwise_Overlap': [0.3032, 0.2481, 0.316, 0.3397, 0.3136, 0.1633, 0.3673, 0.1249, 0.0853],\n",
    "    'Unified_Layerwise_Abs_Correlation': [0.2121, 0.2498, 0.1045, 0.1357, 0.1067, 0.1426, 0.1124, 0.0748, 0.1374],\n",
    "    'Unified_Componentwise_Overlap': [0.2113, 0.1635, 0.4492, 0.5120, 0.4282, 0.1062, 0.4940, 0.1408, 0.1097],\n",
    "    'Unified_Componentwise_Abs_Correlation': [0.2735, 0.2343, 0.0893, 0.2809, 0.0577, 0.1368, 0.0170, 0.0282, 0.0306],\n",
    "    'Unified_Componentwise_Correlation': [0.2735, 0.2343, 0.0893, 0.2809, 0.03999, 0.1368, -0.01128, 0.1328, 0.03068],\n",
    "    'Modular_Final_Loss': [2.74e-6, 1.80e-6, 4.14e-6, 1.19e-6, 1.58e-6, 1.88e-7, 1.23e-6, 9.48e-6, 3.32e-7],\n",
    "    'Modular_Layerwise_Overlap': [0.3731, 0.3285, 0.3716, 0.4088, 0.3513, 0.2185, 0.3943, 0.3289, 0.2095],\n",
    "    'Modular_Layerwise_Abs_Correlation': [0.1959, 0.0919, 0.1252, 0.0530, 0.0509, 0.0874, 0.0752, 0.0740, 0.0481],\n",
    "    'Modular_Componentwise_Overlap': [0.3647, 0.3192, 0.3368, 0.4012, 0.3720, 0.2209, 0.4096, 0.1807, 0.1927],\n",
    "    'Modular_Componentwise_Abs_Correlation': [0.2135, 0.0861, 0.1523, 0.0645, 0.0654, 0.1010, 0.1126, 0.0278, 0.0290],\n",
    "    'Modular_Componentwise_Correlation': [0.0021, -0.0758, 0.0071, 0.0021, -0.0478, -0.0758, -0.04318, -0.0361, -0.02264],\n",
    "    'Scaled_Final_Loss': [1.33e-5, 2.47e-6, 4.71e-6, 6.06e-6, 2.36e-6, 3.67e-7, 1.60e-6, 3.45e-6, 4.38e-7],\n",
    "    'Scaled_Layerwise_Overlap': [0.4287, 0.3818, 0.4261, 0.4423, 0.4735, 0.2897, 0.4771, 0.3497, 0.2544],\n",
    "    'Scaled_Layerwise_Abs_Correlation': [0.1278, 0.1586, 0.1397, 0.1000, 0.0789, 0.1391, 0.0883, 0.0895, 0.0474],\n",
    "    'Scaled_Componentwise_Overlap': [0.4511, 0.3758, 0.4424, 0.4733, 0.4400, 0.2861, 0.4994, 0.2562, 0.2563],\n",
    "    'Scaled_Componentwise_Abs_Correlation': [0.1535, 0.1547, 0.0953, 0.1035, 0.0949, 0.1046, 0.0832, 0.0366, 0.0495],\n",
    "    'Scaled_Componentwise_Correlation': [-0.0063, 0.0681, 0.0043, -0.0063, 0.0432, 0.0681, -0.01724, -0.0274, -0.01182],\n",
    "    'Unified_Epochs_10^-4': [51, 20.6, 16.2, 16.8, 19.6, 15.6, 20.6, 36.8, 33.2],\n",
    "    'Unified_Epochs_10^-5': [130.2, 43.8, 42.4, 43.2, 43.8, 41.2, 39.2, 82.0, 66.2],\n",
    "    'Unified_Epochs_10^-6': [None, 93.4, 84.0, 92.4, 85.4, 75.6, 75.2, 176.4, 132.6],\n",
    "    'Unified_Epochs_10^-7': [None, 258.0, None, None, None, 111.8, None, 264.6, 197.8],\n",
    "    'Modular_Epochs_10^-4': [52.6, 18.4, 24.0, 24.6, 34.0, 33.2, 23.2, 52.0, 43.6],\n",
    "    'Modular_Epochs_10^-5': [122.8, 42.0, 52.0, 45.8, 63.2, 70.4, 51.0, 86.0, 89.2],\n",
    "    'Modular_Epochs_10^-6': [None, 81.0, 91.2, 88.6, 94.2, 116.6, 76.8, 256.8, 188.8],\n",
    "    'Modular_Epochs_10^-7': [None, None, None, None, None, None, None, None, 306.3],\n",
    "    'Scaled_Epochs_10^-4': [90.8, 21.2, 25.6, 25.0, 27.6, 21.2, 26.8, 46.4, 37.8],\n",
    "    'Scaled_Epochs_10^-5': [224.3, 47.4, 58.0, 54.0, 57.6, 51.0, 51.2, 88.6, 78.2],\n",
    "    'Scaled_Epochs_10^-6': [None, 122.0, 219.8, 278.2, 167.0, 88.8, 195.6, 101.3, 150.2],\n",
    "    'Scaled_Epochs_10^-7': [None, None, None, None, None, None, None, 178.3, 262.0]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to create scatter plots with adjusted labels\n",
    "def scatter_plot_with_labels(x, y, title, x_label, y_label, filename):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    texts = []\n",
    "    for label, col, color in zip([\"Unified\", \"Modular\", \"Scaled\"],\n",
    "                                 [\"Unified_\" + y, \"Modular_\" + y, \"Scaled_\" + y],\n",
    "                                 ['blue', 'green', 'red']):\n",
    "        plt.scatter(df[x], df[col], label=label, color=color)\n",
    "        for i, arch in enumerate(df['Architecture']):\n",
    "            texts.append(plt.text(df[x][i], df[col][i], str(arch), fontsize=8))\n",
    "    plt.xscale('log')\n",
    "    if y in [\"Final_Loss\"]:\n",
    "        plt.yscale('log')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    adjust_text(texts, arrowprops=dict(arrowstyle='->', color='gray'))\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "# for y in [\"-4\", \"-5\", \"-6\", \"-7\"]:\n",
    "#     scatter_plot_with_labels(\n",
    "#         'Total_Params',\n",
    "#         f'Epochs_10^{y}',\n",
    "#         f'Convergence Speed vs. Total Parameters for $10^{{{y}}}$',\n",
    "#         'Total Parameters (log scale)',\n",
    "#         f'Average Epochs to Reach $10^{{{y}}}$',\n",
    "#         f'convergence_speed_vs_params_{y}.png'\n",
    "#     )\n",
    "\n",
    "# scatter_plot_with_labels('Total_Params', 'Final_Loss',\n",
    "#                          'Final Loss vs. Total Parameters',\n",
    "#                          'Total Parameters (log scale)',\n",
    "#                          'Final Loss (log scale)',\n",
    "#                          'final_loss_vs_params.png')\n",
    "\n",
    "scatter_plot_with_labels('Total_Params', 'Layerwise_Overlap',\n",
    "                         'Layer-wise Overlap vs. Total Parameters',\n",
    "                         'Total Parameters (log scale)',\n",
    "                         'Layer-wise Overlap',\n",
    "                         'overlap_vs_params.png')\n",
    "\n",
    "scatter_plot_with_labels('Total_Params', 'Layerwise_Abs_Correlation',\n",
    "                         'Layer-wise Abs. Correlation vs. Total Parameters',\n",
    "                         'Total Parameters (log scale)',\n",
    "                         'Layer-wise Abs. Correlation',\n",
    "                         'correlation_vs_params.png')\n",
    "\n",
    "scatter_plot_with_labels('Total_Params', 'Componentwise_Overlap',\n",
    "                         'Component-wise Overlap vs. Total Parameters',\n",
    "                         'Total Parameters (log scale)',\n",
    "                         'Component-wise Overlap',\n",
    "                         'overlap_vs_params.png')\n",
    "\n",
    "scatter_plot_with_labels('Total_Params', 'Componentwise_Abs_Correlation',\n",
    "                         'Component-wise Abs. Correlation vs. Total Parameters',\n",
    "                         'Total Parameters (log scale)',\n",
    "                         'Component-wise Abs. Correlation',\n",
    "                         'correlation_vs_params.png')\n",
    "\n",
    "scatter_plot_with_labels('Total_Params', 'Componentwise_Correlation',\n",
    "                         'Component-wise Correlation vs. Total Parameters',\n",
    "                         'Total Parameters (log scale)',\n",
    "                         'Component-wise Correlation',\n",
    "                         'correlation_vs_params.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Elastic Wave System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Strongly Coupled\n",
    "# strongly_coupled = UnifiedVsModularComparison(ElasticWaveSystemStronglyCoupled())\n",
    "\n",
    "# for (arch, name) in experiments:\n",
    "#     uni_strongly, mod_strongly = strongly_coupled.train_unified_and_modular(500, 1000, 1e-3, unified_architecture=arch, train_scaled=False)\n",
    "\n",
    "#     models = [\n",
    "#         (uni_strongly, \"UnifiedPINN\"),\n",
    "#         (mod_strongly, \"ModularPINN\"),\n",
    "#     ]\n",
    "\n",
    "#     strongly_coupled.analyze_interpretability(models, save_path=f\"ElasticWave_StronglyCoupled_{name}_Interpretability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Directionally Dominant\n",
    "# directionally_dominant = UnifiedVsModularComparison(ElasticWaveSystemDirectionalDominance())\n",
    "\n",
    "# for (arch, name) in experiments:\n",
    "#     uni_dominant, mod_dominant = directionally_dominant.train_unified_and_modular(500, 1000, 1e-3, unified_architecture=arch, train_scaled=False)\n",
    "\n",
    "#     models = [\n",
    "#         (uni_dominant, \"UnifiedPINN\"),\n",
    "#         (mod_dominant, \"ModularPINN\"),\n",
    "#     ]\n",
    "\n",
    "#     directionally_dominant.analyze_interpretability(models, save_path=f\"ElasticWave_DirectionalDominance_{name}_Interpretability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Anisotropic\n",
    "# anisotropic = UnifiedVsModularComparison(ElasticWaveSystemAnisotropic())\n",
    "\n",
    "# for (arch, name) in experiments:\n",
    "#     uni_anisotropic, mod_anisotropic = anisotropic.train_unified_and_modular(500, 1000, 1e-3, unified_architecture=arch, train_scaled=False)\n",
    "\n",
    "#     models = [\n",
    "#         (uni_anisotropic, \"UnifiedPINN\"),\n",
    "#         (mod_anisotropic, \"ModularPINN\"),\n",
    "#     ]\n",
    "\n",
    "#     anisotropic.analyze_interpretability(models, save_path=f\"ElasticWave_Anisotropic_{name}_Interpretability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title SemiModularElasticWave Network\n",
    "class SemiModularElasticWavePINN(tf.keras.Model):\n",
    "    def __init__(self, displacement_architecture=(128, 64), stress_architecture=(128, 64, 32), activation='tanh'):\n",
    "        super(SemiModularElasticWavePINN, self).__init__()\n",
    "\n",
    "        # Displacement sub-network (u_x, u_y)\n",
    "        self.displacement_hidden_layers = [\n",
    "            tf.keras.layers.Dense(units, activation=activation) for units in displacement_architecture\n",
    "        ]\n",
    "        self.displacement_output_layer = tf.keras.layers.Dense(4)  # Outputs: u_x, u_y\n",
    "\n",
    "        # Stress sub-network (sigma_xx, sigma_yy, sigma_xy)\n",
    "        self.stress_hidden_layers = [\n",
    "            tf.keras.layers.Dense(units, activation=activation) for units in stress_architecture\n",
    "        ]\n",
    "        self.stress_output_layer = tf.keras.layers.Dense(1)  # Outputs: sigma_xx, sigma_yy, sigma_xy\n",
    "\n",
    "    def call(self, x_points, t_points):\n",
    "        combined_input = tf.concat([t_points, x_points], axis=1)\n",
    "\n",
    "        # Displacement sub-network\n",
    "        out_displacement = combined_input\n",
    "        for layer in self.displacement_hidden_layers:\n",
    "            out_displacement = layer(out_displacement)\n",
    "        displacement_outputs = self.displacement_output_layer(out_displacement)\n",
    "\n",
    "        # Stress sub-network\n",
    "        out_stress = combined_input\n",
    "        for layer in self.stress_hidden_layers:\n",
    "            out_stress = layer(out_stress)\n",
    "        stress_outputs = self.stress_output_layer(out_stress)\n",
    "\n",
    "        # Combine outputs\n",
    "        return tf.concat([displacement_outputs, stress_outputs], axis=1)\n",
    "\n",
    "    @property\n",
    "    def hidden_layers(self):\n",
    "        # Combine displacement and stress hidden layers for unified access\n",
    "        return self.displacement_hidden_layers + self.stress_hidden_layers\n",
    "\n",
    "\n",
    "comp = UnifiedVsModularComparison(ElasticWaveSystem())\n",
    "semi_modular_model = SemiModularElasticWavePINN(\n",
    "    displacement_architecture=(256, 256),\n",
    "    stress_architecture=(256, 256)\n",
    ")\n",
    "unified_model = UnifiedPINN(\n",
    "    output_dim=5,\n",
    "    architecture=(256, 256),\n",
    "    activation='tanh'\n",
    ")\n",
    "modular_model = ModularPINNs(\n",
    "    num_components=5,\n",
    "    architectures=[(256, 256)] * 5,\n",
    "    activation='tanh'\n",
    ")\n",
    "\n",
    "\n",
    "# Train the semi-modular model\n",
    "loss_history_semi, component_losses_semi, training_time_semi = comp.system.train(\n",
    "    semi_modular_model, epochs=5, N=1000, lr=1e-3\n",
    ")\n",
    "loss_history_uni, component_losses_uni, training_time_uni = comp.system.train(\n",
    "    unified_model, epochs=5, N=1000, lr=1e-3\n",
    ")\n",
    "loss_history_mod, component_losses_mod, training_time_mod = comp.system.train(\n",
    "    modular_model, epochs=5, N=1000, lr=1e-3\n",
    ")\n",
    "\n",
    "UnifiedVsModularComparison.compare_overall_loss(loss_history_uni, loss_history_mod, loss_history_semi)\n",
    "UnifiedVsModularComparison.compare_component_losses(component_losses_uni, component_losses_mod, component_losses_semi)\n",
    "\n",
    "comp = UnifiedVsModularComparison(ElasticWaveSystem())\n",
    "comp.analyze_interpretability([(semi_modular_model, \"SemiModularPINN\"), (unified_model, \"UnifiedPINN\"), (modular_model, \"ModularPINN\")], save_path=\"SemiModularElasticWave_Interpretability\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
